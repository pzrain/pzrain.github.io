---
layout: default
title: 2019C
parent: Paper Analysis
grand_parent: COMAP
permalink: /COMAP/paper/2019C
---

# 2019 Problem C
* TOC
{:toc}

## Problem：[The Opioid Crisis](https://www.mathmodels.org/Problems/2019/MCM-C/2019_MCM_Problem_C.pdf)

### 题目背景

美国正在经历一场关于使用合成和非合成**阿片类药物**的全国性危机。

阿片类药物不仅对人的健康有极大的负面影响，并且，如果阿片类药物蔓延到人口的各个阶层，许多需要高素质的技术岗位可能再难找到合适的人选。

国家法医实验室信息系统（NFLIS）发布了一份数据，在本问题中，我们只关注其中涉及到五个州（Ohio，Kentucky，West Virginia，Virginia，Tennessee）的部分。

### 提供的数据

1. $$\mathtt{MCM\_NFLIS\_DATA.xlsx}$$：包含2010年至2017年这五个州的每个县的麻醉镇痛药（合成阿片类药物）和海洛因的药物鉴定计数（也即每个县每年阿片类药物的**使用量**）。
2. $$\mathtt{ACS\_xx\_5YR\_DP02.zip}$$：为美国人口普查局（U.S. Sensus Bureau）的摘录，这些摘录记录了2010-2016年每年为这五个州的县收集的一组共同的**社会经济因素**。



### 问题

1. ***Question 1***：使用提供的$$\mathtt{NFLIS}$$数据

   * 建立一个数学模型来**描述**随着时间的推移，五个州及其县之间报告的阿片类药物事件的**传播**和**特征**。

   * 使用建立的模型，确定**每个州**可能开始使用阿片类药物的**位置**。

   * 假设建立的模型正确，是否有哪些**特征**是美国政府需要特别关注的？并且这些特征是会在阿片类药物使用数量达到什么**阈值**时才会出现？以及你的模型预测它们会在未来的**什么时间**、**什么地点**出现？

     > 提取特征，研究特征出现的条件、时间和地点

2. ***Question2***：使用提供的U.S. Census Bureau的数据

   已经有很多假设来解释是什么导致了阿片的使用和成瘾，以及哪些群体在使用阿片。

   * 阿片类药物的使用或是其使用的趋势是否与数据中提供的某些**社会经济因素**有关？
   * 如果是，修改1中的模型，加入这些重要的影响因素。

3. ***Question 3***

   * 基于1与2中的结果，为应对阿片危机提供一种**可行的策略**。
   * 使用模型来验证策略的**有效性**。
   * 研究策略获得成功或失败所依赖的重要**参数**（及其范围）。（敏感性分析）

4. ***1-2 page memo*** 来总结建模过程中提出的重要观点以及成果。



## Paper #1906204

### 论文框架与思路

1. **数据集预处理**

   数据集$$\mathtt{MCM\_NFLIS\_DATA.xlsx}$$中共囊括了**69**种不同的阿片类药物以及位于五个州中的**462**个县。分析发现，许多阿片类药物在部分县中的使用量极小，因此使用$$\mathtt{K-MEANS}$$算法将县划分为更大一点的区（Zone）。实际过程中，设置$$k=100$$，最终每个区中大约包括2\~8个县。

   > 实际上就是用距离远近来对各个县进行划分，感觉使用$$\mathtt{K-MEANS}$$稍微有些牵强。

2. **提出假设**

   * 阿片类药物的传播趋势与时间有关，并且**时间越远**的趋势应当具有更高的权重（原因在于，其距离起源的时间点越近）
   * 药物的传播只会在相邻的县之间进行，而不会跨越一段很长的距离

3. **随机游走模型确定阿片类药物的源头**

   * **模型定义**

     定义有向图$$G=(V,E)$$。其中，$$v_i\in V$$代表1中划分的zone，点的权重$$W_i$$为该区域内使用**某一种**阿片类药物的人数。$$(v_i,v_j)\in E$$当且仅当$$v_i$$，$$v_j$$对应的zone地理距离小于一个**阈值**（200km）并且$$W_i>W_j$$。定义边的权重为$$\mathcal{sim}(v_i,v_j)=p_{v_i\rightarrow v_j}$$，也即两个区的“**相似度**”：

     $$
     \mathcal{sim}(v_i,v_j)=\sum_{k=2010}^{2016}v^{k-1}\cdot\mathcal{Corr}(X_{i,k},X_{j,k})
     $$

     其中，$$X_{i,k}$$表示表示区域$$i$$在第$$k$$年的特征向量，也即一个$$1\times69$$的向量，每一维度表示这一年该区域使用对应种类阿片的数量。$$\mathcal{Corr}$$表示相关系数。$$v(=0.7)$$是一个时间衰减因子，原论文中公式如上，但是$$v$$的指数似乎取为$$(k-2010)$$更为合适？

   * **模型实现**

     选择有向图中出度最大的5个点作为源头的候选点，以其为源头进行100次随机模拟，记录节点$$v_i$$的访问次数$$n_i$$，最终计算候选点的得分如下：

     $$
     \mathbf{score_i}=\sum_{0\leq j\leq99,j\neq i}n_j\cdot W_i\cdot p_{v_i\rightarrow v_j}
     $$
     
     得分最高的候选点为模型选出的针对该种药物的源头。

   * **结果**

     1. 交通因素：一些源头位于海边或者大湖边。
     2. 法律因素：俄亥俄州的阿片类药物源头较多，可能是因为该州立法系统存在漏洞。

4. **SVM确定药物使用的阈值，SVR对药物使用量进行预测**

   论文中选择研究的特征似乎就是某一区域“是否处于阿片危机”，对于这一特征，其选择两个值作为指标，一个是总的药物使用量，一个是阿片类药物的使用量，前者作为纵坐标，后者作为横坐标，从而可以用二维平面上的一个点来代表一个区域。

   1. 使用one-class-SVM来确定哪些区域“处于阿片危机”。这一步的作用实际上是给每个区域打上标签，也即是/不是处于阿片危机。one-class-SVM是非监督的，打上标签后便可以采用监督学习中的SVM模型来对这些二维平面上的点进行分类。

   2. 使用SVM进行分类，获得决策边界也即**药物使用的阈值**，超过这一阈值，便认为该区域处于阿片危机。SVM所做的，实际上是得到这样一条线，将二维平面上的点分成两个部分，使得处于同一类的点尽量位于这条线的同一侧，这条线就叫做决策边界。
      
      $$
      w^T\phi(x)=0
      $$


   3. 使用SVR（*Support Machine Regression*）对2018、2019年各个地区的阿片使用量进行预测，使用grid search方法进行调参。其中，每个区域的预测结果需要根据相似性，结合其他区域的预测结果。具体地，论文中给出的算法是去除与当前区域相似度最高的十个区域，计算这些区域的表征，并用这些表征来分别做出预测，最后根据预测结果对这些表征进行加权求和。

   论文中得到了几个结论：

   1. 阿片的传播具有**地域性**。原先就处于阿片危机中的区域，其附近的区域更有可能在未来面临阿片危机。
   2. 沿海、沿湖地区阿片问题更加严重，这可能是因为这些地区的**人口密度**往往更大。、

5. **三步确定相关联的社会经济因素**

   1. 使用**Association rule learning**来确定关联的社会经济因素

      > Association rule learning要求每个变量取值都是0/1之一，论文中在这里把所有社会经济变量都二值化，感觉有些强行使用这一算法的意味(ーー゛)……

      所谓的*Association rule*，是指形如以下的规则：

      $$
      (A_1,A_2)\Rightarrow A_3
      $$

      表示当$$A_1,A_2$$同时出现时，$$A_3$$更有可能出现。*Association rule learning*所作的，就是在庞大的数据集寻找这些规则，数据集中包含许多变量（每一地区的社会经济因素），以及我们有该地区是否处于阿片危机的指标。定义变量的集合为$$X=\{v_1,v_2,\cdots,v_n\}$$，定义一个集合的支持度（*support*）如下：
      
      $$
      \mathcal{supp}(X)=\frac{|{t\in T;X\subseteq t}|}{|T|}
      $$
      
      其中，$$T$$表示所有的事件集合。$$\mathcal{sup}(X)$$所计算的，实际上就是$$X$$中的所有变量全部在某一次事件中出现的概率。根据这一概率的大小，我们可以定义$$X$$为*frequent set*，当且仅当$$\mathcal{supp}(X)$$超过某一阈值。如果直接遍历所有集合计算*support*的值，算法复杂度会达到$$O(2^n)$$，这是不可接受的。因此，可以采用*Apriori*算法来降低算法复杂度，从而得到一系列*frequent set*。进而由这些*frequent set*可以得到一系列规则，计算规则的*confidence level*如下：
      
      $$
      \mathcal{conf}(X\Rightarrow Y)=\frac{\mathcal{supp}(X\cup Y)}{\mathcal{supp}(X)}
      $$
      
      同样给*confidence level*设定一个阈值，可以筛选出一些规则，这样就得到了和是否处于阿片危机相关的一些因素。

      > *Apriori*算法实际上只有两个关键点：如果一个集合$$X$$是*frequent set*，那么它的所有子集也一定都是*frequent set*；而如果一个集合非*frequent set*，那么它的所有超集也非*frequent set*。

      最终分析结果时，论文中提到共选取出了八个社会经济因素，包括*Femal householder with no husband present*等。

      > 注：论文中此处的分析非常含糊，仅仅是给出了最终结果。结合之前对方法的描述，怀疑可能是在得到的众多规则中人为选了一个可解释性较强的。本身这一方法不确定的程度就过高，实现起来难度比较大。

   2. 使用相关系数从a中得到的八个因素中筛选出对处于阿片危机正相关的变量，最终筛出了三个，分别是*Households with one or more people 65 years and over*，*Graduate or professional degree*，*Family households(families)-Female householder with no husband present*。

   3. 再使用$$\mathtt{PCA}$$将三个特征降维成一个，也即只提取第一个主成分，这个特征会被送进之前的模型中，作为“与社会经济因素相关的输入参数”。

      > 感觉这里$$\mathtt{PCA}$$的加入实在有点没有必要，论文中说的是*too large for input for model introduced above. So we use PCA to project three features into one line*，但要把三个向量变成一行直接做向量之间的连接貌似就可以了吧，本身向量维数就不高，$$\mathtt{PCA}$$还会丢失很多信息。

6. **提出策略**

   1. 针对5中得出的三个特征（这里又选用三个特征而不是$$\mathtt{PCA}$$降维后的一个特征了……），分别进行分析，例如第二个因素，学历更高的人一般来说更明白阿片类药物的危害，对阿片类药物上瘾的概率更低。
   2. 提出策略：论文中分别针对老年人、受教育水平、以及第三个因素中的失去丈夫的家庭主妇提出了建议，例如政府应当给公民提供更多受教育的机会。此外还简单写了一些别的策略，但主要是针对三个因素来提策略的。
   3. 对策略有效性的验证：也是针对提出的三种策略，调整参数分别进行验证，采用控制变量法。具体的，例如降低受教育程度较高的人群的比例，观察模型最终预测的该地区阿片使用量的变化趋势。

7. **敏感性分析**：论文中这一部分比较简略，提到了几个参数，包括3中定义的距离阈值（200km），以及3中对时间衰减因子的选择，并且提了一下5中描述很含糊的那个部分（但是感觉似乎也并没有说到什么实质性的东西）。
8. **Memo**：把整篇文章各个部分的结果给总结了一下，包括阿片的起源地、处于阿片危机的区域的预测、相关联的社会经济因素的提取，以及提出的政策。

### 优点与缺点

1. 对研究层次的调整是很关键的，并不是研究每个县，而是将多个县聚合成一个区域。
2. 用了一些类比。例如，描述阿片类药物传播的层次型模式时类比浏览网页时，点开页面上的一个链接，在跳转的页面上继续选择感兴趣的链接点击，以此类推；描述选择相似度的原因时借用了推荐系统（Recommendation System）的概念。以及研究社会经济因素时，把每个城市想象成一个“*shopping basket*”。
3. 图片的可视化做的很充分，基本上每一步中间结果都有对应的可视化图片，~~虽然感觉有的可视化效果一般，解释的也没有很到位。~~
4. 个人觉得对一些概念的描述有些不够清晰，比如对于节点出度的描述，论文中没有用*out degree*，而是先描述了节点的度的定义，然后说“Here we change the definition to  the number of edges *scatter* from the vertex”，有些奇怪。还有些别的概念似乎也是如此。调参时使用的其实是机器学习中常用的grid search方法，但也没有说清楚。但有些概念的使用也很准确，比如对社会经济因素的数据集做的binarize操作。叙述上可能可以更专业一点。
5. 总的来说这篇论文结构以及使用的方法还是很复杂的，这也使得论文中某些部分的条理性不是非常清晰。最后结果的呈现肯定花了很多时间在数据集上做筛选，才得到一个解释性比较强的结果，能在短时间内做到这个程度我觉得已经挺厉害了。